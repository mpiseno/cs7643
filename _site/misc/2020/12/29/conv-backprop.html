<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Backpropagation in Convolutional Layers | Michael Piseno’s Deep Learning Blog</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Backpropagation in Convolutional Layers" />
<meta name="author" content="Michael Piseno" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In CS 7643, we’ve been teaching backprop through conv layers with a sets of hand-written notes by professor Dhruv Batra ever since I’ve been a TA, and allegedly since 2015. The content in this post is based heavily on these hand-written notes and Dhruv’s lectures - I am merely making it more accessible by posting here. Here it goes." />
<meta property="og:description" content="In CS 7643, we’ve been teaching backprop through conv layers with a sets of hand-written notes by professor Dhruv Batra ever since I’ve been a TA, and allegedly since 2015. The content in this post is based heavily on these hand-written notes and Dhruv’s lectures - I am merely making it more accessible by posting here. Here it goes." />
<link rel="canonical" href="http://localhost:4000/cs7643/misc/2020/12/29/conv-backprop.html" />
<meta property="og:url" content="http://localhost:4000/cs7643/misc/2020/12/29/conv-backprop.html" />
<meta property="og:site_name" content="Michael Piseno’s Deep Learning Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-29T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"http://localhost:4000/cs7643/misc/2020/12/29/conv-backprop.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/cs7643/misc/2020/12/29/conv-backprop.html"},"author":{"@type":"Person","name":"Michael Piseno"},"description":"In CS 7643, we’ve been teaching backprop through conv layers with a sets of hand-written notes by professor Dhruv Batra ever since I’ve been a TA, and allegedly since 2015. The content in this post is based heavily on these hand-written notes and Dhruv’s lectures - I am merely making it more accessible by posting here. Here it goes.","headline":"Backpropagation in Convolutional Layers","datePublished":"2020-12-29T00:00:00-05:00","dateModified":"2020-12-29T00:00:00-05:00","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/cs7643/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/cs7643/feed.xml" title="Michael Piseno's Deep Learning Blog" /><script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script></head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/cs7643/">Michael Piseno&#39;s Deep Learning Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Backpropagation in Convolutional Layers</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-12-29T00:00:00-05:00" itemprop="datePublished">Dec 29, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In CS 7643, we’ve been teaching backprop through conv layers with a sets of hand-written notes by professor Dhruv Batra ever since I’ve been a TA, and allegedly since 2015. The content in this post is based heavily on these hand-written notes and Dhruv’s lectures - I am merely making it more accessible by posting here. Here it goes.</p>

<h2 id="notation-and-assumptions">Notation and Assumptions</h2>

<p>First we will formalize some notation. We assume that the layer has input $\mathbf{X} \in \mathbb{R}^{N_{1} \times N_{2}}$, a kernel $\mathbf{w} \in \mathbb{R}^{k_{1} \times k_{2}}$, and output $\mathbf{y} \in \mathbb{R}^{N_{1} \times N_{2}}$ (i.e. assume that we have sufficient padding to make the output the same dimension as the input). We also assume that the stride is 1. For simplcity, we are not including the channel dimension in the input or kernel (i.e. $c = 1$). Finally, $\mathbf{X}$ and $\mathbf{y}$ will be indexed by $(r, c)$ and $\mathbf{w}$ will be indexed by $(a, b)$.</p>

<p>Recall how the $(r, c)$ entry of the output is computed in the forward pass:</p>

<script type="math/tex; mode=display">\mathbf{y}[r, c] = \sum_{a=0}^{k_{1}-1} \sum_{b=0}^{k_{2}-1} \mathbf{X}[r + a, c + b] \mathbf{w}[a, b]</script>

<div class="post-img">
    <img src="/cs7643/assets/img/conv-backprop/conv-forward.png" />
</div>

<p>Recall in backprop at a given layer, we receive an upstream gradient $\frac{\partial L}{\partial \mathbf{y}}$ and we need to compute $\frac{\partial L}{\partial \mathbf{w}}$ and $\frac{\partial L}{\partial \mathbf{X}}$ for the backward pass.</p>

<div class="post-img">
    <img src="/cs7643/assets/img/conv-backprop/conv-layer.png" style="width: 40%" />
</div>

<p>Let’s start with $\frac{\partial L}{\partial \mathbf{w}}$.</p>

<h2 id="gradient-of-mathbfw">Gradient of $\mathbf{w}$:</h2>

<p>$\frac{\partial L}{\partial \mathbf{w}}$ is the same size as $\mathbf{w}$. We will consider computing the gradient at 1 entry in the kernel, i.e. $\frac{\partial L}{\partial \mathbf{w}[a’, b’]}$. We also have to incorporate the upstream gradient $\frac{\partial L}{\partial \mathbf{y}}$ that we are given. We can do this by asking the question: “how do slight changes to $\mathbf{w}[a’, b’]$ affect the output $\mathbf{y}$?”. Well, if we recall the forward pass, every pixel of $\mathbf{y}$ is a dot product of the kernel $\mathbf{w}$ with some region of the input, so small changes to $\mathbf{w}[a’, b’]$ will affect every pixel of the output! So we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
    \frac{\partial L}{\partial \mathbf{w}[a', b']} &= \sum_{p \in \mathbf{y}} \frac{\partial L}{\partial p} \frac{\partial p}{\partial \mathbf{w}[a', b']} \\
    &= \sum_{r=0}^{N_{1}-1} \sum_{c=0}^{N_{2}-1} \frac{\partial L}{\partial \mathbf{y}[r, c]} \frac{\partial \mathbf{y}[r, c]}{\partial \mathbf{w}[a', b']}
\end{align*} %]]></script>

<p>We already know $\frac{\partial L}{\partial \mathbf{y}[r, c]}$. To compute $\frac{\partial \mathbf{y}[r, c]}{\partial \mathbf{w}[a’, b’]}$ we can use the formula for the forward pass.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\frac{\partial}{\partial \mathbf{w}[a', b']} \mathbf{y}[r, c] &= \frac{\partial}{\partial \mathbf{w}[a', b']} \sum_{a=0}^{k_{1}-1} \sum_{b=0}^{k_{2}-1} \mathbf{X}[r + a, c + b] \mathbf{w}[a, b] \\
&= \sum_{a=0}^{k_{1}-1} \sum_{b=0}^{k_{2}-1} \frac{\partial}{\partial \mathbf{w}[a', b']} \mathbf{X}[r + a, c + b] \mathbf{w}[a, b] \\
&= \mathbf{X}[r + a', c + b']
\end{align*} %]]></script>

<p>We can also arrive at the same conclusion by thinking about what pixel of $\mathbf{X}$ gets multiplied by $\mathbf{w}[a’, b’]$ when computing $\mathbf{y}[r, c]$. Visually:</p>

<div class="post-img">
    <img src="/cs7643/assets/img/conv-backprop/conv-dLdw.png" />
</div>

<p>It should be clear that $\frac{\partial \mathbf{y}[r, c]}{\partial \mathbf{w}[a’, b’]} = \mathbf{X}[r + a’, c + b’]$. So, finally we have</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{w}[a', b']} = \sum_{r=0}^{N_{1}-1} \sum_{c=0}^{N_{2}-1} \frac{\partial L}{\partial \mathbf{y}[r, c]} \mathbf{X}[r + a', c + b']</script>

<p>So the gradient w.r.t. $\mathbf{w}$ is in fact a convolution between $\frac{\partial L}{\partial \mathbf{y}}$ and $\mathbf{X}$! Of course, since $\frac{\partial L}{\partial \mathbf{y}}$ and $\mathbf{X}$ are the same size, convolving them would yield a scalar, so it is actually a convolution with $\mathbf{X}$ padded such that the result of the convolution is size $k_{1} \times k_{2}$, because that is the size of $\frac{\partial L}{\partial \mathbf{w}}$.</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{w}} = \mathbf{X}_{\text{padded}} * \frac{\partial L}{\partial \mathbf{y}}</script>

<h2 id="gradient-of-mathbfx">Gradient of $\mathbf{X}$:</h2>

<p>Just like last time, let’s compute the gradient one pixel at a time. How do small changes to the pixel $\mathbf{X}[r’, c’]$ affect $\mathbf{y}$? Recall that one propety of convolutional layers is local connectivity, meaning a subset of the input is connected to a subset of the output (rather than dense connectivity in fully-connected layers). This means that $\mathbf{X}[r’, c’]$ is connected to some region in $\mathbf{y}$. How can we mathematically define this region?</p>

<p>As the kernel passes over $\mathbf{X}$, it passes over the pixel $\mathbf{X}[r’, c’]$ at some position and starts using it to compute output pixels. It passes over the pixel $\mathbf{X}[r’, c’]$ for the last time when computing output pixel $\mathbf{y}[r’, c’]$. Visually:</p>

<div class="post-img">
    <img src="/cs7643/assets/img/conv-backprop/conv-dLdX.gif" />
</div>

<p>So computing the derivative w.r.t. $\mathbf{X}[r’, c’]$ amounts to summing over the region in $\mathbf{y}$ that depends on $\mathbf{X}[r’, c’]$. Let’s call that region $\mathbf{R}_{r’, c’} \subset \mathbf{y}$. From the image above, we can see</p>

<script type="math/tex; mode=display">\mathbf{R}_{r', c'} = \mathbf{y}[\max(0, r' - k_{1} + 1), \max(0, c' - k_{2} + 1)]</script>

<p>And so,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\frac{\partial L}{\partial \mathbf{X}[r', c']} &= \sum_{p \in \mathbf{R}_{r', c'}} \frac{\partial L}{\partial p} \frac{\partial p}{\partial \mathbf{X}[r', c']} \\
&= \sum_{a=0}^{k_{1}-1}\sum_{b=0}^{k_{2}-1} \frac{\partial L}{\partial \mathbf{y}[\max(0, r'-a), \max(0,c'-b)]} \frac{\partial \mathbf{y}[\max(0, r'-a), \max(0,c'-b)]}{\partial \mathbf{X}[r', c']}
\end{align*} %]]></script>

<p>The max function prevents us from indexing out of bounds in $\mathbf{y}$. Let’s omit it for clarity, where it is understood that any negative index should be discarded. So we are left with</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial \mathbf{X}[r', c']} = \sum_{a=0}^{k_{1}-1}\sum_{b=0}^{k_{2}-1} \frac{\partial L}{\partial \mathbf{y}[r'-a, c'-b]} \frac{\partial \mathbf{y}[r'-a,c'-b]}{\partial \mathbf{X}[r', c']}</script>

<p>All that’s left is to compute $\frac{\partial \mathbf{y}[r’-a,c’-b]}{\partial \mathbf{X}[r’, c’]}$. We can do this analytically. Recall</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
    \mathbf{y}[r', c'] &= \sum_{\alpha=0}^{k_{1}-1} \sum_{\beta=0}^{k_{2}-1} \mathbf{X}[r' + \alpha, c' + \beta] \mathbf{w}[\alpha, \beta]
\end{align*} %]]></script>

<p>So</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
    \mathbf{y}[r' - a, c' - b] &= \sum_{\alpha=0}^{k_{1}-1} \sum_{\beta=0}^{k_{2}-1} \mathbf{X}[r' - a + \alpha, c' - b + \beta] \mathbf{w}[\alpha, \beta] \\
    \Rightarrow \frac{\partial}{\partial \mathbf{X}[r', c']} \mathbf{y}[r' - a, c' - b] &= \frac{\partial}{\partial \mathbf{X}[r', c']} \sum_{\alpha=0}^{k_{1}-1} \sum_{\beta=0}^{k_{2}-1} \mathbf{X}[r' - a + \alpha, c' - b + \beta] \mathbf{w}[\alpha, \beta] \\
    &= \sum_{\alpha=0}^{k_{1}-1} \sum_{\beta=0}^{k_{2}-1} \frac{\partial}{\partial \mathbf{X}[r', c']} \mathbf{X}[r' - a + \alpha, c' - b + \beta] \mathbf{w}[\alpha, \beta] \\
    &= \mathbf{w}[a, b]
\end{align*} %]]></script>

<p>Now we can plus this result back into our equation for the derivative w.r.t. $\mathbf{X}[r’, c’]$.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
    \frac{\partial L}{\partial \mathbf{X}[r', c']} &= \sum_{a=0}^{k_{1}-1}\sum_{b=0}^{k_{2}-1} \frac{\partial L}{\partial \mathbf{y}[r'-a, c'-b]} \frac{\partial \mathbf{y}[r'-a, c'-b]}{\partial \mathbf{X}[r', c']} \\
    &= \sum_{a=0}^{k_{1}-1}\sum_{b=0}^{k_{2}-1} \frac{\partial L}{\partial \mathbf{y}[r'-a, c'-b]} \mathbf{w}[a, b]
\end{align*} %]]></script>

<h2 id="closing-remarks">Closing Remarks</h2>

<p>Hopefully this post was a helpful and concise explanation of backprop in conv layers. I am purposefully omitting code since that’s part of the homework for 7643. This is also good practice for defining the backward pass of other layers, which we will have to do a lot in CS 7643. Anyway, email me with questions, comments, passionate rants, and the like. Stay hydrated y’all.</p>


  </div><a class="u-url" href="/cs7643/misc/2020/12/29/conv-backprop.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/cs7643/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Michael Piseno&#39;s Deep Learning Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name"><li><a class="u-name" href="http://www.michaelpiseno.com" target="_blank">Michael Piseno</a></li></li><li>mpiseno at gatech dot edu</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a rel="me" href="https://github.com/mpiseno" title="mpiseno"><svg class="svg-icon grey"><use xlink:href="/cs7643/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/mpiseno" title="mpiseno"><svg class="svg-icon grey"><use xlink:href="/cs7643/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul></div>

      <div class="footer-col footer-col-3">
        <p>I post resources for the course I TA, CS 7643: Deep Learning, as well as stuff I think is cool.
</p>
      </div>
    </div>

  </div>

</footer></body>

</html>